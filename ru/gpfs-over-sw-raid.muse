#title GPFS поверх software raid

Если вы не в курсе что такое GPFS, то информация, расположенная ниже, скорее всего, является для вас
бесполезной.

* Зачем это?

В GPFS имеется ограничение на максимальное количество реплик одного файла, равное двум. Это значит,
что при создании файловой системы на обычных дисках (без шареного стораджа), при потере двух
дисков, есть ненулевая вероятность потерять данные. При росте количества дисков, эта вероятность,
само собой, растет, что совершенно неприкольно.

Большие дядьки обычно используют для этого шареные стораджи на FC, но простым людям тоже хочется
как-то жить, не опасаясь за свои данные. В этом случае можно использовать RAID на каждой
storage-ноде GPFS.

* Как правильно заюзать MD

Чтобы GPFS выполнял ввод-вывод через MD, а не через низлежащие устройства, необходимо ему об этом
сказать. Нужно это по причине особенностей поиска GPFS своих томов. 

Поиск заключается в том, что GPFS по очереди считывает первый сектор со всех дисков, упомянутых в
/proc/partition. Если в первом секторе находится сигнатура существующего NSD, то GPFS считает что
может использовать это блочное устройство для выполнения ввода-вывода. Это очень удобно, когда
дурной linux может изменять имена дисков между перезагрузками, а также для больших стораджей, когда
один и тот же кусок стораджа может быть виден несколькими разными путями с несколькаих машин.

Однако, это создает проблему при использовании raid1 (да и при raid0-raid10 могут возникнуть глюки,
если с размером страйпа не повезет). Чтобы этого избежать, нужно объяснить GPFS на каких дисках
искать злосчастные метки. Это осущестляется через создание скрипта /var/mmfs/etc/nsddevices,
примерно с таким содержимым:
<example>
#!/bin/sh

for i in /dev/md*; do
  echo $i generic
done
exit 0
</example>

То есть, этот скрипт должен выводить в stdout список имен устройств со специальным словом generic
(для AIX тут возможны варианты, для x86 -- только generic). После этого, GPFS будет выполнять поиск
NSD только среди блочных девайсов, которые выводит скрипт.

* Проблема с BIO MD

Симптомы: при попытке создать файловую систему на NSD-дисках, являющихся софтварным рейдом, mmcrfs
зависает навсегда.

Небольшое исследование показало следующую последовательность событий:

  1. mmcrfs выполняется несколько проверок и, если все хорошо, запускает tscrfs.
  1. tscrfs создает сокет, через msgqueue сообщает демону порт. Соединение устанавливается, tscrfs
   говорит демону "создай ФС"
  1. демон выполняет ioctl 0x1b (27) на файл /dev/ss0
  1. этот ioctl проваливается в функцию модуля mmfs ss_ioctl, затем в DiskSched::synchIO
  4. synchIO создает IOWaitQueue (cxiAllocIOWaitQueue), на которой выполняется cxiWaitIO
  5. создается BIO, по завершении которого вызывается фукнция bioDone. В ней написано вот что:
<example>
  /* Ignore if bi_size is still non-zero */
  if (bioP->bi_size)
#if LINUX_KERNEL_VERSION >= 2062700
    return;
#else
    return 1;
#endif
</example>
  Именно эта проверка и срабатывает в случае md. Для обычных блочных устройств в поле bi_size честный
  ноль.

Так как поле bi_size означает "сколько еще осталось байт обработать в этом BIO", то это бага MD/DM
(не дурость ли вызывать функцию оповещающую о завершении BIO, не обновляя bi_size). Буду разбираться
дальше, пока есть вот такой патч, с которым GPFS начинает нормально работать с
MD-устройствами. Строго говоря, патч не совсем корректен, но как временное решение сойдет:

<example>
diff -Nru src.orig/gpl-linux/cxiIOBuffer.c src/gpl-linux/cxiIOBuffer.c
--- src.orig/gpl-linux/cxiIOBuffer.c    2009-09-04 14:39:34.000000000 +0400
+++ src/gpl-linux/cxiIOBuffer.c 2009-09-04 14:39:43.000000000 +0400
@@ -1570,14 +1570,6 @@
 {
   struct cxiBufHeadChunk_t* bhcHeadP;

-  /* Ignore if bi_size is still non-zero */
-  if (bioP->bi_size)
-#if LINUX_KERNEL_VERSION >= 2062700
-    return;
-#else
-    return 1;
-#endif
-
   bhcHeadP = (struct cxiBufHeadChunk_t*)bioP->bi_private;
   /* Decrement number of outstanding bios */
   /* If this is the last bio in the chunk list, enqueue the chunk list
</example>
